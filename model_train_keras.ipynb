{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_train_keras.ipynb","private_outputs":true,"provenance":[{"file_id":"1bAHUEGuPzpwRptjpNuNcsjoFpkflOlmv","timestamp":1630635660557}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"4fmFhqzmYG9X"},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import os\n","import cv2\n","import torch.optim as optim\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D, InputLayer\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import optimizers\n","from tensorflow.keras import Sequential\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kum1W16hbX5J"},"source":["ls_counts = 0\n","labels = np.zeros(5000)\n","for i in range(1,41):\n","  if i<10:\n","    i='0'+str(i)\n","  for i, _, k in os.walk('/content/drive/Shareddrives/high_classification/train/'+str(i)):  # store labels\n","    imgs = sorted(k)\n","    for filename in imgs:\n","      if str(filename)[-2] == 'p':\n","        img = cv2.imread(str(i)+'/'+str(filename))\n","        with open(str(i)+'/'+str(filename).replace('.jpg','.hytag'), 'r') as f:\n","          while True:                \n","            line = f.readline()\n","            if line:\n","                line = line.strip().split(',')\n","            \n","                # ROI coordinate extraction\n","                hytag_list = line \n","                x=int(hytag_list[4])\n","                y=int(hytag_list[5])\n","                w=int(hytag_list[6])\n","                h=int(hytag_list[7]) \n","                # label extraction\n","                \n","                if hytag_list[0] == 'LBUID_636802211114365653_0529': # Edema\n","                    labels[ls_counts] = 1\n","                elif hytag_list[0] == 'LBUID_636802210898995628_5475': # Normal \n","                    labels[ls_counts] = 0\n","\n","                roi = img[y:y+h, x:x+w]\n","                \n","                roi_resize = cv2.resize(roi, (64, 64))\n","                roi_resize = roi_resize[: ,:, 0]\n","                roi_resize_ravel = roi_resize.ravel()\n","                roi_resize_ravel = roi_resize_ravel.reshape(-1, 64, 64, 1)\n","\n","                ls_counts += 1\n","\n","                if ls_counts < 2:\n","                    roi_all = roi_resize_ravel\n","                else:\n","                    roi_all = np.vstack((roi_all, roi_resize_ravel))\n","            else:\n","                break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhQ6lKciq1hz"},"source":["labels = labels[:len(roi_all)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5FFzzkPMcdd"},"source":["ls_counts = 0\n","t_labels = np.zeros(5000)\n","for i in range(41,51):\n","  for i, _, k in os.walk('/content/drive/Shareddrives/high_classification/train/'+str(i)):  # store labels\n","    imgs = sorted(k)\n","    for filename in imgs:\n","      if str(filename)[-2] == 'p':\n","        img = cv2.imread(str(i)+'/'+str(filename))\n","        with open(str(i)+'/'+str(filename).replace('.jpg','.hytag'), 'r') as f:\n","          while True:                \n","            line = f.readline()\n","            if line:\n","                line = line.strip().split(',')\n","            \n","                # ROI coordinate extraction\n","                hytag_list = line \n","                x=int(hytag_list[4])\n","                y=int(hytag_list[5])\n","                w=int(hytag_list[6])\n","                h=int(hytag_list[7]) \n","                # label extraction\n","                \n","                if hytag_list[0] == 'LBUID_636802211114365653_0529': # Edema\n","                    t_labels[ls_counts] = 1\n","                elif hytag_list[0] == 'LBUID_636802210898995628_5475': # Normal \n","                    t_labels[ls_counts] = 0\n","\n","                roi = img[y:y+h, x:x+w]\n","                \n","                roi_resize = cv2.resize(roi, (64, 64))\n","                roi_resize = roi_resize[: ,:, 0]\n","                roi_resize_ravel = roi_resize.ravel()\n","                roi_resize_ravel = roi_resize_ravel.reshape(-1, 64, 64, 1)\n","\n","                ls_counts += 1\n","\n","                if ls_counts < 2:\n","                    roi_all_t = roi_resize_ravel\n","                else:\n","                    roi_all_t = np.vstack((roi_all_t, roi_resize_ravel))\n","            else:\n","                break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UftmGZCtNCeM"},"source":["t_labels = t_labels[:len(roi_all_t)]\n","print(len(labels))\n","print(len(t_labels))\n","print(roi_all.shape)\n","print(roi_all_t.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5x14WJivOtmN"},"source":["np.save('train.npy',roi_all)\n","np.save('labels.npy',labels)\n","np.save('test.npy',roi_all_t)\n","np.save('t_labels.npy',t_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1F2UeBgcW8N"},"source":["\"\"\"\n","---------------- 여기부터 Train ---------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThXyAnyGYvx7"},"source":["x_train = np.load('/content/drive/Shareddrives/high_classification/data/train_x.npy')\n","y_train = np.load('/content/drive/Shareddrives/high_classification/data/train_y.npy')\n","x_valid = np.load('/content/drive/Shareddrives/high_classification/data/validation_x.npy')\n","y_valid = np.load('/content/drive/Shareddrives/high_classification/data/validation_y.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wc4oAQPXsOX"},"source":["from sklearn.utils import shuffle \n","\n","y_train = y_train.astype(np.uint8)\n","y_valid = y_valid.astype(np.uint8)\n","\n","x_train, y_train = shuffle(x_train, y_train, random_state=13)\n","x_valid, y_valid = shuffle(x_valid, y_valid, random_state=13)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l_W9jQl4b3Yd"},"source":["x_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NReBxQ6KcQLL"},"source":["x_train[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90-8G5HIb7ls"},"source":["a = np.zeros((64, 64, 3))\n","a[:, :, 0] = x_train[1].reshape((64, 64, 1))\n","# a[:, :, 1] = x_train[2]\n","# a[:, :, 2] = x_train[3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EuM1OSM8dk_W"},"source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_valid.shape)\n","print(y_valid.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMUhFMCVVL04"},"source":["# class CNN(nn.Module):\n","#   def __init__(self):\n","#     super(CNN,self).__init__()\n","#     self.layer1 = nn.Sequential(\n","#         nn.Conv2d(1,3,5),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(4))\n","#     self.layer2 = nn.Sequential(\n","#         nn.Conv2d(3,6,4),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(6))\n","#     self.layer3 = nn.Sequential(\n","#         nn.Linear(6*10*10,300),\n","#         nn.ReLU(),\n","#         nn.Linear(300,2))\n","\n","#   def forward(self, x):\n","#     out = self.layer1(x)\n","#     out = self.layer2(out)\n","#     out = out.view(6*10*10, -1)\n","#     out = self.layer3(out)\n","#     return out\n","\n","# keras model\n","def CNN_base_model(learning_rate=1e-3):\n","\n","    model = Sequential()\n","    \n","    model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 1)))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(64, (3, 3)))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(64, (3, 3)))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(0.25))\n","    \n","    model.add(Flatten())  # 3d array from CNN layer to 1d array \n","    model.add(Dense(64))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.25))\n","    \n","    model.add(Dense(1))\n","    model.add(Activation('sigmoid'))\n","\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer=optimizers.SGD(),\n","                  metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZdbKvjJDeoHc"},"source":["# net = CNN()\n","# print(net)\n","\n","model = CNN_base_model()\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-76azhoCUhy_"},"source":["# criterion = nn.CrossEntropyLoss()\n","# optimizer = optim.SGD(net.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WcsAqxkXGSY"},"source":["# loader = DataLoader(train)\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n","batch_size = 16\n","\n","train_datagen = ImageDataGenerator(rescale=1./255)  # Normalization\n","valid_datagen = ImageDataGenerator(rescale=1./255)  # Normalization\n","\n","train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size, seed=13)\n","valid_generator = valid_datagen.flow(x_valid, y_valid, batch_size=batch_size, seed=13)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Dlh2YBKXnN_"},"source":["# for epoch in range(20):   \n","\n","#     running_loss = 0.0\n","#     for data in loader:\n","#         optimizer.zero_grad()\n","#         outputs = net(data.reshape(1,1,64, 64))\n","#         loss = criterion(outputs, train_labels)\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         running_loss += loss.item()\n","#         print('[Epoch:{}] cost = {}'.format(epoch+1, running_loss))\n","\n","\n","epochs = 100\n","\n","# mc = ModelCheckpoint('mc/0709-all-ori-SGD.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n","\n","history = model.fit(\n","    train_generator,\n","    epochs=epochs,\n","    validation_data=valid_generator,\n","    validation_steps=int(x_valid.shape[0] / batch_size),\n","    steps_per_epoch= int(x_train.shape[0] / batch_size),)\n","    # callbacks=[mc]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nD6G6u9Hft8L"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PlkwnEufdas"},"source":[""],"execution_count":null,"outputs":[]}]}